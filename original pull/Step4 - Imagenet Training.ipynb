{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "Having implemented and tested all the components of the final networks in steps 1-3, we are now ready to train the network on a large dataset (ImageNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from libs.pconv_model import PConvUnet\n",
    "from libs.util import random_mask\n",
    "\n",
    "#from IPython.display import display \n",
    "#from pil import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.ioff()\n",
    "\n",
    "# SETTINGS\n",
    "TRAIN_DIR = r\"D:\\ILSVRC\\Data\\CLS-LOC\\traintest\\\\\"\n",
    "TEST_DIR = r\"D:\\ILSVRC\\Data\\CLS-LOC\\test\\\\\"\n",
    "VAL_DIR = r\"D:\\ILSVRC\\Data\\CLS-LOC\\val\\\\\"\n",
    "\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train & test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(ImageDataGenerator):\n",
    "    def flow_from_directory(self, directory, *args, **kwargs):\n",
    "        generator = super().flow_from_directory(directory, class_mode=None, *args, **kwargs)\n",
    "        while True:\n",
    "            \n",
    "            # Get augmentend image samples\n",
    "            ori = next(generator)\n",
    "\n",
    "            # Get masks for each image sample\n",
    "            mask = np.stack([random_mask(ori.shape[1], ori.shape[2]) for _ in range(ori.shape[0])], axis=0)\n",
    "\n",
    "            # Apply masks to all image sample\n",
    "            masked = deepcopy(ori)\n",
    "            masked[mask==0] = 1\n",
    "\n",
    "            # Yield ([ori, masl],  ori) training batches\n",
    "            # print(masked.shape, ori.shape)\n",
    "            gc.collect()\n",
    "            yield [masked, mask], ori\n",
    "            \n",
    "# Create training generator\n",
    "train_datagen = DataGenerator(  \n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR, target_size=(512, 512), batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create validation generator\n",
    "val_datagen = DataGenerator(rescale=1./255)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR, target_size=(512, 512), batch_size=BATCH_SIZE, seed=1\n",
    ")\n",
    "\n",
    "# Create testing generator\n",
    "test_datagen = DataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR, target_size=(512, 512), batch_size=BATCH_SIZE, seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out an example\n",
    "test_data = next(test_generator)\n",
    "(masked, mask), ori = test_data\n",
    "\n",
    "# Show side by side\n",
    "for i in range(len(ori)):\n",
    "    _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    axes[0].imshow(masked[i,:,:,:])\n",
    "    axes[1].imshow(mask[i,:,:,:] * 1.)\n",
    "    axes[2].imshow(ori[i,:,:,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_callback(model):\n",
    "    \"\"\"Called at the end of each epoch, displaying our previous test images,\n",
    "    as well as their masked predictions and saving them to disk\"\"\"\n",
    "    \n",
    "    # Get samples & Display them        \n",
    "    pred_img = model.predict([masked, mask])\n",
    "    pred_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "    # Clear current output and display test images\n",
    "    for i in range(len(ori)):\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        axes[0].imshow(masked[i,:,:,:])\n",
    "        axes[1].imshow(pred_img[i,:,:,:] * 1.)\n",
    "        axes[2].imshow(ori[i,:,:,:])\n",
    "        axes[0].set_title('Masked Image')\n",
    "        axes[1].set_title('Predicted Image')\n",
    "        axes[2].set_title('Original Image')\n",
    "                \n",
    "        plt.savefig(r'data/test_samples/img_{}_{}.png'.format(i, pred_time))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 - with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = PConvUnet(weight_filepath='data/logs/')\n",
    "#model.load(r\"C:\\Users\\MAFG\\Documents\\Github-Public\\PConv-Keras\\data\\logs\\50_weights_2018-06-01-16-41-43.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run training for certain amount of epochs\n",
    "model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=10000,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=100,\n",
    "    epochs=50,        \n",
    "    plot_callback=plot_callback,\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir='../data/logs/initial_training', write_graph=False)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 - without batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from previous run\n",
    "model = PConvUnet(weight_filepath='data/logs/')\n",
    "model.load(\n",
    "    #r\"C:\\Users\\MAFG\\Documents\\Github-Public\\PConv-Keras\\data\\logs\\150_weights_2018-06-26-22-19-32.h5\",\n",
    "    r\"C:\\Users\\bhaley\\Desktop\\homework\\419\\FinalProj\\ML-Final-Project\\original pull\\data\\logs\\50_weights_2018-11-14-03-48-39.h5\",\n",
    "    train_bn=False,\n",
    "    lr=0.00005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run training for certain amount of epochs\n",
    "model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=10000,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=100,\n",
    "    epochs=20,        \n",
    "    plot_callback=plot_callback,\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir='../data/logs/fine_tuning', write_graph=False)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Phase 3 - Generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from previous run\n",
    "model = PConvUnet(weight_filepath='data/logs/')\n",
    "model.load(\n",
    "    r\"C:\\Users\\bhaley\\Desktop\\homework\\419\\FinalProj\\ML-Final-Project\\original pull\\data\\logs\\70_weights_2018-11-16-15-07-15.h5\",\n",
    "    train_bn=False,\n",
    "    lr=0.00005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for (masked, mask), ori in tqdm(test_generator):\n",
    "    \n",
    "    # Run predictions for this batch of images\n",
    "    pred_img = model.predict([masked, mask])\n",
    "    pred_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    # Clear current output and display test images\n",
    "    for i in range(len(ori)):\n",
    "        _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(masked[i,:,:,:])\n",
    "        axes[1].imshow(pred_img[i,:,:,:] * 1.)\n",
    "        axes[0].set_title('Masked Image')\n",
    "        axes[1].set_title('Predicted Image')\n",
    "        axes[0].xaxis.set_major_formatter(NullFormatter())\n",
    "        axes[0].yaxis.set_major_formatter(NullFormatter())\n",
    "        axes[1].xaxis.set_major_formatter(NullFormatter())\n",
    "        axes[1].yaxis.set_major_formatter(NullFormatter())\n",
    "                \n",
    "        plt.savefig(r'data/test_samples/img_{}_{}.png'.format(i, pred_time))\n",
    "        plt.close()\n",
    "        n += 1\n",
    "        \n",
    "    # Only create predictions for about 100 images\n",
    "    if n > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
